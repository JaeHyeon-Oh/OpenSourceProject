{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "testenv",
   "display_name": "TestKernel",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime,timedelta,date\n",
    "import pickle, progressbar, json, glob, time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "sleep_sec = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Utility\n",
    "def date_range(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date + timedelta(1)).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "def parse_date(s):\n",
    "    return datetime.strptime(s, '%Y.%m.%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press Crawler\n",
    "press_list = ['연합뉴스','KBS','매일경제','MBC','SBS','JTBC']\n",
    "def crawling_main_text(url):\n",
    "    req = requests.get(url)\n",
    "    req.encoding = None\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    try:\n",
    "        # 연합뉴스\n",
    "        if ('://yna' in url) | ('app.yonhapnews' in url): \n",
    "            main_article = soup.find('div', {'class':'story-news article'})\n",
    "            if main_article == None:\n",
    "                main_article = soup.find('div', {'class' : 'article-txt'})\n",
    "            text = main_article.text\n",
    "        # MBC \n",
    "        elif '//imnews.imbc' in url: \n",
    "            text = soup.find('div', {'itemprop' : 'articleBody'}).text\n",
    "        # 매일경제(미라클), req.encoding = None 설정 필요\n",
    "        elif 'mirakle.mk' in url:\n",
    "            text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "        # 매일경제, req.encoding = None 설정 필요\n",
    "        elif 'mk.co' in url:\n",
    "            text = soup.find('div', {'class' : 'art_txt'}).text\n",
    "        # SBS\n",
    "        elif 'news.sbs' in url:\n",
    "            text = soup.find('div', {'itemprop' : 'articleBody'}).text\n",
    "        # KBS\n",
    "        elif 'news.kbs' in url:\n",
    "            text = soup.find('div', {'id' : 'cont_newstext'}).text\n",
    "        # JTBC\n",
    "        elif 'news.jtbc' in url:\n",
    "            text = soup.find('div', {'class' : 'article_content'}).text\n",
    "        else:\n",
    "            raise Exception()\n",
    "    except:\n",
    "        return None\n",
    "    return text.replace('\\n','').replace('\\r','').replace('<br>','').replace('\\t','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naver Crawler\n",
    "query = input('검색할 키워드: ')\n",
    "news_num_per_day = int(input('일당 수집 뉴스의 수(숫자만 입력): '))\n",
    "date_start = parse_date(input('시작일(YYYY.MM.DD): '))\n",
    "date_end = parse_date(input('종료일((YYYY.MM.DD): '))\n",
    "\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "\n",
    "print('브라우저를 실행시킵니다(자동 제어)\\n')\n",
    "chrome_path = '/usr/bin/chromedriver'\n",
    "browser = webdriver.Chrome(chrome_path)\n",
    "\n",
    "news_list=[]\n",
    "for date in date_range(date_start,date_end):\n",
    "    news_url = 'https://search.naver.com/search.naver?where=news&query={0}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={1}'.format(query,date)\n",
    "    #news_url = 'https://search.naver.com/search.naver?where=news&query={}'.format(query)\n",
    "    #headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "    browser.get(news_url)#,headers=headers)\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "\n",
    "    ######### 언론사 선택 및 confirm #####################\n",
    "    print('설정한 언론사를 선택합니다.\\n')\n",
    "    search_opt_box = browser.find_element_by_xpath('//*[@id=\"search_option_button\"]')\n",
    "    if search_opt_box.get_attribute('aria-pressed')=='false':\n",
    "        search_opt_box.click()\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    # 언론사 선택하는 바를 활성화\n",
    "    tablist_box = browser.find_element_by_xpath('//div[@class=\"snb_inner\"]/ul[@role=\"tablist\" and @class=\"option_menu\"]')\n",
    "\n",
    "    tablist_elem_list = tablist_box.find_elements_by_xpath('./li[@role=\"presentation\"]')\n",
    "    press_box = [t for t in tablist_elem_list if t.text == '언론사'][0].find_element_by_xpath('./a')\n",
    "    press_box.click()\n",
    "\n",
    "\n",
    "    # 언론사 종류 하나씩 선택\n",
    "    actived_press_frame = browser.find_element_by_xpath('.//div[@class=\"snb_itembox lst_press _search_option_press_\"]')\n",
    "    total_press_box = actived_press_frame.find_element_by_xpath('./div[@class=\"group_sort type_press _group_by_press_\"]')\n",
    "\n",
    "    # 언론사 종류를 선택하는 버튼이 담긴 박스\n",
    "    press_cat_active_button = total_press_box.find_elements_by_xpath('.//a[@role=\"tab\" and @class=\"item _tab_filter_\"]') # 언론사 종류 하나씩 버튼\n",
    "    press_cat_active_button_dict = dict(zip([t.text for t in press_cat_active_button], press_cat_active_button)) # 언론사 종류 이름 : 언론사 종류 활성화 버튼\n",
    "\n",
    "    # 밑에 각 언론사 종류별 개별 언론사가 담겨있는 박스들\n",
    "    each_press_box_list = total_press_box.find_elements_by_xpath('.//div[@class=\"scroll_area _panel_filter_\"]')\n",
    "\n",
    "    # 1. 언론사 종류 1개 선택\n",
    "    # 2. 선택한 언론사 종류에 해당하는 개별 언론사 중 크롤링할 언론사에 포함되는 것 체크 \n",
    "    for idx, press_cat_name in enumerate(press_cat_active_button_dict.keys()):\n",
    "        #하나의 언론사 종류를 클릭해서 활성화시킴\n",
    "        press_cat_active_button_dict[press_cat_name].click()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        # 선택한 언론사 종류 안의 개별 언론사가 담긴 박스\n",
    "        each_press_box = each_press_box_list[idx].find_element_by_xpath('./div[@class=\"select_item\"]')\n",
    "        # 개별 언론사의 이름\n",
    "        each_press_title_list = [ep.get_attribute('title') for ep in each_press_box.find_elements_by_xpath('.//label')]\n",
    "        # 개별 언론사 체크 박스\n",
    "        each_press_input_list = each_press_box.find_elements_by_xpath('.//input')\n",
    "        \n",
    "\n",
    "        # 딕셔너리(개별 언론사 이름 : 개별 언론사 체크 박스)\n",
    "        each_press_title_input_dict = dict(zip(each_press_title_list, each_press_input_list))\n",
    "        # 추출하고 싶은 언론사 존재 시 체크박스 클릭\n",
    "        for title in [tit for tit in each_press_title_input_dict.keys() if tit in press_list]:\n",
    "            print(title)\n",
    "            each_press_title_input_dict[title].click()\n",
    "    # 확인 버튼\n",
    "    confirm_buttons = actived_press_frame.find_element_by_xpath('./span[@class=\"btn_inp\"]').find_elements_by_xpath('.//button')\n",
    "    ok_button = [c for c in confirm_buttons if c.text == '확인'][0]\n",
    "    ok_button.click()\n",
    "\n",
    "\n",
    "    ################ 뉴스 크롤링 ########################\n",
    "\n",
    "    print('\\n크롤링을 시작합니다.')\n",
    "    time.sleep(sleep_sec)\n",
    "    # ####동적 제어로 페이지 넘어가며 크롤링\n",
    "    idx = 1\n",
    "    cur_page = 1\n",
    "\n",
    "    pbar = tqdm(total=news_num_per_day)\n",
    "        \n",
    "    while idx < news_num_per_day:\n",
    "        table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "        li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "        area_list = [li.find_element_by_xpath('.//div[@class=\"news_area\"]') for li in li_list]\n",
    "        a_list = [area.find_element_by_xpath('.//a[@class=\"news_tit\"]') for area in area_list]\n",
    "\n",
    "        for n in a_list[:min(len(a_list), news_num_per_day-idx+1)]:\n",
    "            n_url = n.get_attribute('href')\n",
    "            text = crawling_main_text(n_url)\n",
    "            if text==None:\n",
    "                continue\n",
    "            news_list.append({'title' : n.get_attribute('title'), \n",
    "                            'url' : n_url,\n",
    "                            'text' : text})\n",
    "            idx += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "        if idx < news_num_per_day:\n",
    "            cur_page +=1\n",
    "\n",
    "            pages = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n",
    "            next_page_url = [p for p in pages.find_elements_by_xpath('.//a') if p.text == str(cur_page)][0].get_attribute('href')\n",
    "\n",
    "            browser.get(next_page_url)\n",
    "            time.sleep(sleep_sec)\n",
    "        else:\n",
    "            pbar.close()\n",
    "            time.sleep(0.7)\n",
    "            break\n",
    "\n",
    "print('\\n브라우저를 종료합니다.\\n' + '=' * 100)\n",
    "browser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "news_df = DataFrame(dict(enumerate(news_list))).T\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "xlsx_file_name = '네이버뉴스_{}.csv'.format(query)\n",
    "\n",
    "news_df.to_csv(xlsx_file_name)\n",
    "\n",
    "print('엑셀 저장 완료 | 경로 : {}\\\\{}\\n'.format(folder_path, xlsx_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}